# ðŸŽ® VTuberAI - Anna the Otaku Gamer Chatbot

**Anna** is a customizable AI assistant that listens, talks, sees, and searches. Designed as a voice-interactive gaming companion, Anna responds intelligently using speech recognition, text-to-speech, web search, and visual context from your screen.

Anna is the generic customizable version of Kira-chan, the VTuber created by KryptykBioz.

https://github.com/KryptykBioz

https://www.youtube.com/@KryptykBioz

---

## ðŸ§  Features

- ðŸŽ¤ **Voice Recognition** via Vosk  
- ðŸ—£ï¸ **Text-to-Speech** using a virtual audio cable (VB-Cable)  
- ðŸ’¬ **Conversational AI** using Ollama LLMs  
- ðŸ§  **Long-term memory** with FAISS + embeddings  
- ðŸ” **Web Search Agent** to provide up-to-date info  
- ðŸ‘ï¸ **Vision Context** with screenshot-to-LLM prompts  
- ðŸ•¹ï¸ **Persona**: Set by the user at initialization
- ðŸ” **Summarization and Memory Pruning** for persistent context  

---

## ðŸ› ï¸ Requirements

- Python 3.10+  
- [Ollama](https://ollama.com) installed locally  
- VB-Cable for audio loopback (used for lip-sync/OBS)  
- Vosk model downloaded for voice recognition  
- Environment variables (or `.env` file) for configuration  

---

## âš™ï¸ Configuration (`Config` class)

Environment variables (or defaults):

| Variable                | Purpose                                                   |
|-------------------------|-----------------------------------------------------------|
| `LLM_MODEL`             | The main chatbot LLM (e.g. `qwen2.5:3b-instruct`)         |
| `EMBED_MODEL`           | Embedding model for FAISS (e.g. `nomic-embed-text`)       |
| `VISION_MODEL_NAME`     | Name of vision-capable LLM (e.g. `llava:7b-v1.5`)         |
| `VISION_MODEL_ENDPOINT` | HTTP endpoint for image+text inference                    |
| `VECTOR_STORE_PATH`     | Directory for long-term memory FAISS index               |
| `PAST_CONVO_FILE`       | Initial text log to seed vector memory                   |
| `BACKUP_FILE`           | Serialized `.pkl` backup of conversation history          |
| `SUMMARY_TRIGGER`       | Exchanges before triggering memory summarization          |

---

## ðŸ§© Core Tools and Libraries

| Tool/Library            | Function                                                     |
|-------------------------|--------------------------------------------------------------|
| `vosk`                  | Offline speech-to-text using Kaldi                           |
| `pyttsx3 + VB-Cable`    | Text-to-speech piped to OBS or avatar software               |
| `OllamaLLM` (LangChain) | Access to local LLMs (e.g. Mistral, LLaVA, Qwen)             |
| `OllamaEmbeddings`      | Embeddings for FAISS via local models                        |
| `FAISS`                 | Vector database for semantic memory                          |
| `LangChain`             | Prompt templating, memory integration, and chaining          |
| `pyautogui`             | Screenshots for vision input                                 |
| `requests`              | HTTP calls to vision model                                   |
| `search_agent`          | Web search and article parsing module                        |
| `text_to_voice`         | Custom TTS output through VB-Cable                           |
| `voice_to_text`         | Vosk model loader utility                                    |

---

## ðŸ§  Memory System

- **Short-Term Memory**: Maintained within the running instance.  
- **Long-Term Memory**: Stored as semantic vectors using FAISS.  
- **Summarization**: After `N` interactions, a summary is generated and embedded.  
- **Pruning**: Keeps the vector index size bounded with recent summaries prioritized.  

---

## ðŸ” Web Search Integration

- Triggered automatically on each prompt.  
- Fetches top 3 articles, extracts summaries, and appends context to the LLM prompt.  

---

## ðŸ‘ï¸ Vision Capabilities

- Activated when keywords like `see`, `screen`, or `look` are present.  
- Captures a screenshot using `pyautogui`.  
- Sends base64-encoded image + prompt to a local LLaVA endpoint.  
- Result is inserted as `[Visual Context]` in the prompt.  

---

## ðŸ—£ï¸ Voice Loop Interaction

1. Audio stream from mic is processed by Vosk.  
2. User commands are queued and combined.  
3. When idle or prompted, the message is sent to the LLM.  
4. Reply is voiced using TTS and stored in long-term memory.  
5. Periodically, summaries are generated to reduce vector size.  

---

## ðŸ§ª Sample Prompt Template

```text
{system_prompt}

[Chat History]
{memory}

[Search Results]
{search_results_section}

[Visual Context]
{vision_section}

[User Input]
{user_input}

Anna:
```



## ðŸ§™ Persona

Anna's personality is set by the user and by interacting with the user. 

A system_prompt may be edited and sent along with each prompt for context.

The initMemory.txt may have a sample chat log manually written into it to 

establish the bot's personality and establish its preferred response format

and behavior


## ðŸ—‚ï¸ File Structure (Key Modules)

â”œâ”€â”€ models/                # Contains Vosk Models 

â”œâ”€â”€ main.py                # Entry point with VTuberAI class

â”œâ”€â”€ voice_to_text.py       # Vosk model loader

â”œâ”€â”€ text_to_voice.py       # TTS via VB-Cable

â”œâ”€â”€ search_agent.py        # Web search agent

â”œâ”€â”€ initMemory.txt          # Optional initial conversation log for the training and basis of memory

â”œâ”€â”€ long_term_store/       # Vector store for memory (FAISS)

â”œâ”€â”€ chat_backup.pkl        # Optional serialized chat history for backup memory

â”œâ”€â”€ START.ps1              # Right-click -> Run with Powershell (as administrator) -> Launches bot in terminal

â””â”€â”€ README.md              # You're reading it!



## ðŸ“¦ Future Ideas
- OBS webcam & lipsync integration

- Emotion detection and affective responses

- Multimodal visual QA with real-time game stream

- Richer summarization with memory replay




# COMPLETE SETUP INSTALLATION GUIDE FOR DIFFERENT VERSIONS

## ðŸš€ GETTING STARTED FROM SCRATCH (Basic Chat Bot)

Install Python
Install Ollama


### 1. Install requirements
pip install -r requirements.txt

### 2. Run your local models via Ollama
ollama run qwen2.5:3b-instruct
ollama run nomic-embed-text
ollama run mistral:7b-instruct
ollama run llava:7b-v1.5
Open a Windows Terminal and 'serve ollama'

### 3. Start the chatbot
Open a second terminal and start virtual environment, then run 'python main.py'
OR
Right-click main.py -> Run with Powershell (as administrator) -> Launches bot in terminal

### FIRST RUN
If a memory file has been created (initMemory.txt), the first run may have to embed each line of 

the document to establish the bot's memory and personality. This may take a while depending on how

many lines were written into this file. Do not interrupt this process. The Ollama terminal will

output a line for each embedding to show that it is running. When it is done, the Bot's terminal

will show that it is ready for input to start the chat.


## ðŸš€ SET UP TALK MODE (chat using voice-to-text, text-to-voice)

Install Vosk and KaldiRecognizer


## ðŸš€ AVATAR IMPLEMENTATION (Basic Chat Bot)

Install VRoidStudio (to create the avatar) and Warudo (to animate the avatar)
Install VB-Cable virtual cable (to pipe audio to the avatar when bot speaks)


## ðŸš€ SET UP VISION (Vision Mode [see what is on screen])

Install llava:7b-v1.5


## ðŸ‘¤ Author
KryptykBioz â€” VTuber AI developer and chaotic genius behind Anna.

https://github.com/KryptykBioz

https://www.youtube.com/@KryptykBioz